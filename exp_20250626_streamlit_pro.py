# -*- coding: utf-8 -*-
"""

"""
import streamlit as st
import pandas as pd, json, datetime as dt
from pathlib import Path
from sklearn.cluster import *
from sklearn.metrics import adjusted_rand_score
import plotly.express as px
import os
import numpy as np

# åˆå§‹åŒ–ç›®å½•
RESULTS_DIR = Path("results"); RESULTS_DIR.mkdir(exist_ok=True)
LOG = RESULTS_DIR / "log.json"

# ---- å·¥å…·å‡½æ•° ----
def get_subdirectories(path):
    """è·å–æŒ‡å®šè·¯å¾„ä¸‹çš„å­ç›®å½•åˆ—è¡¨"""
    try:
        if os.path.exists(path):
            return [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]
        return []
    except:
        return []

def find_matching_paths(root_path, current_input, max_suggestions=5):
    """æŸ¥æ‰¾åŒ¹é…çš„è·¯å¾„ä½œä¸ºè‡ªåŠ¨è¡¥å…¨å»ºè®®"""
    if not current_input:
        return []
    
    parts = current_input.rstrip(os.sep).split(os.sep)
    if len(parts) <= 1:
        base_path = root_path
        search_term = current_input
    else:
        base_path = os.sep.join(parts[:-1])
        search_term = parts[-1]
    
    while not os.path.exists(base_path) and base_path != "":
        base_path = os.path.dirname(base_path)
    
    subdirs = get_subdirectories(base_path)
    matches = [d for d in subdirs if d.lower().startswith(search_term.lower())]
    
    full_matches = []
    for match in matches:
        if base_path == "":
            full_matches.append(match)
        else:
            full_matches.append(os.path.join(base_path, match))
    
    return full_matches[:max_suggestions]

@st.cache_data
def load_data(dataset_name, dataset_path):
    """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ•°æ®é›†"""
    try:
        file_path = Path(dataset_path) / f"{dataset_name}.csv"
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        df = pd.read_csv(file_path)
        return df.iloc[:, :-1], df.iloc[:, -1]
    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®å‡ºé”™: {str(e)}")
        return None, None

def ensure_utf8(s):
    """å¼ºåˆ¶å°†æ•°æ®è½¬æ¢ä¸ºUTF-8ç¼–ç çš„å­—ç¬¦ä¸²"""
    if isinstance(s, bytes):
        # å°è¯•å¤šç§ç¼–ç è§£ç 
        for encoding in ['utf-8', 'iso-8859-1', 'gbk', 'utf-16']:
            try:
                return s.decode(encoding, errors='replace')
            except:
                continue
        return s.decode('utf-8', errors='replace')
    elif isinstance(s, (np.ndarray, list, tuple)):
        return type(s)(ensure_utf8(item) for item in s)
    elif isinstance(s, dict):
        return {ensure_utf8(k): ensure_utf8(v) for k, v in s.items()}
    else:
        return str(s)

def clean_log_data(data):
    """æ¸…æ´—æ—¥å¿—æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹å¯è½¬ä¸ºUTF-8"""
    cleaned = {}
    for key, value in data.items():
        try:
            # ç‰¹åˆ«å¤„ç†numpyç±»å‹
            if isinstance(value, np.generic):
                cleaned[key] = ensure_utf8(value.item())
            else:
                cleaned[key] = ensure_utf8(value)
        except Exception as e:
            cleaned[key] = f"[ç¼–ç é”™è¯¯: {str(e)}]"
    return cleaned

def log_run(info: dict):
    """è®°å½•è¿è¡Œæ—¥å¿—ï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºUTF-8ç¼–ç """
    try:
        # æ¸…æ´—æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹å¯ç¼–ç ä¸ºUTF-8
        cleaned_info = clean_log_data(info)
        cleaned_info["time"] = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # è¯»å–ç°æœ‰æ—¥å¿—ï¼ˆä½¿ç”¨å®½å®¹çš„ç¼–ç å¤„ç†ï¼‰
        log = []
        if LOG.exists():
            with open(LOG, 'r', encoding='utf-8', errors='replace') as f:
                try:
                    log = json.load(f)
                except json.JSONDecodeError:
                    st.warning("æ—¥å¿—æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†åˆ›å»ºæ–°æ—¥å¿—")
        
        # æ·»åŠ æ–°è®°å½•
        log.append(cleaned_info)
        
        # å†™å…¥æ—¥å¿—ï¼ˆä½¿ç”¨UTF-8ç¼–ç ï¼Œç¡®ä¿ASCII=Falseï¼‰
        with open(LOG, 'w', encoding='utf-8') as f:
            json.dump(log, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"è®°å½•æ—¥å¿—å‡ºé”™: {str(e)}")
        # åº”æ€¥ï¼šå°†æ—¥å¿—ç›´æ¥ä¿å­˜ä¸ºæ–‡æœ¬
        with open(LOG.with_suffix('.txt'), 'a', encoding='utf-8') as f:
            f.write(f"[{dt.datetime.now()}] {str(cleaned_info)}\n")

def read_logs():
    """è¯»å–æ—¥å¿—æ–‡ä»¶ï¼Œè‡ªåŠ¨å¤„ç†ç¼–ç é—®é¢˜"""
    if not LOG.exists():
        return pd.DataFrame()
    
    # å°è¯•è¯»å–æ—¥å¿—ï¼Œä½¿ç”¨é”™è¯¯æ›¿æ¢æ¨¡å¼
    with open(LOG, 'r', encoding='utf-8', errors='replace') as f:
        try:
            log_data = json.load(f)
            return pd.DataFrame(log_data)
        except json.JSONDecodeError:
            st.error("æ—¥å¿—æ–‡ä»¶æ ¼å¼æŸåï¼Œå·²å°è¯•ä¿®å¤")
            # å°è¯•ä¿®å¤æŸåçš„JSON
            fixed_data = []
            for line in f:
                try:
                    fixed_data.append(json.loads(line))
                except:
                    continue
            return pd.DataFrame(fixed_data) if fixed_data else pd.DataFrame()

# ---- Streamlit ç•Œé¢ ----
st.set_page_config(page_title="Clustering Dashboard", layout="wide")
tab_main, tab_history = st.tabs(["ğŸš€ è¿è¡Œå®éªŒ", "ğŸ“œ å†å²è®°å½•"])

# åˆå§‹åŒ–session_state
if 'dataset_path' not in st.session_state:
    st.session_state.dataset_path = "D:\\lzk\\@PG\\03.@G2S2\\è®ºæ–‡ä¿®æ”¹\\0620\\å¯¹æ¯”ç®—æ³•\\Dataset\\UCI"

with tab_main:
    st.header("æ–°å®éªŒ")
    
    with st.sidebar:
        st.subheader("æ•°æ®é›†è®¾ç½®")
        st.markdown("### æ•°æ®é›†æ ¹ç›®å½•")
        
        root_input = st.text_input(
            "",
            value=st.session_state.dataset_path,
            key="dataset_path_input",
            help="è¾“å…¥æ•°æ®é›†æ ¹ç›®å½•ï¼Œæ”¯æŒè‡ªåŠ¨è¡¥å…¨"
        )
        
        if root_input:
            suggestions = find_matching_paths(
                os.path.dirname(st.session_state.dataset_path), 
                root_input
            )
            if suggestions:
                st.markdown("**è‡ªåŠ¨è¡¥å…¨å»ºè®®:**")
                for i, suggestion in enumerate(suggestions):
                    if st.button(f"é€‰æ‹©: {suggestion}", key=f"path_suggestion_{i}"):
                        st.session_state.dataset_path = suggestion
    
    col1, col2 = st.columns(2)
    try:
        dataset_files = [f.stem for f in Path(st.session_state.dataset_path).glob("*.csv")]
        dataset = col1.selectbox("æ•°æ®é›†", dataset_files if dataset_files else ["iris", "glass", "Lsun"])
    except:
        dataset = col1.selectbox("æ•°æ®é›†", ["iris", "glass", "Lsun"])
    
    algo    = col1.selectbox("ç®—æ³•", ["KMeans", "DBSCAN"])
    
    if algo == "KMeans":
        k = col2.slider("ç°‡æ•°", 2, 10, 3)
        params = {"n_clusters": k}
    else:
        eps = col2.slider("eps", 0.1, 5.0, 0.5)
        ms  = col2.slider("min_samples", 1, 20, 5)
        params = {"eps": eps, "min_samples": ms}

    if st.button("å¼€å§‹"):
        X, y = load_data(dataset, st.session_state.dataset_path)
        if X is None:
            st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶")
            st.stop()
        
        if algo == "KMeans":
            labels = KMeans(**params).fit_predict(X)
        else:
            labels = DBSCAN(**params).fit_predict(X)
        
        ari = adjusted_rand_score(y, labels)
        run_time = dt.datetime.now().strftime("%Y-%m-%d %H%M%S")
        
        fig = px.scatter(
            x=X.iloc[:,0], y=X.iloc[:,1], color=labels.astype(str),
            title=f"{dataset}-{algo}", labels={"x":"ç‰¹å¾1", "y":"ç‰¹å¾2"}
        )
        st.plotly_chart(fig, use_container_width=True)
        
        st.metric("ARI (è°ƒæ•´å…°å¾·æŒ‡æ•°)", f"{ari:.4f}")
        
        fname = RESULTS_DIR / f"{dataset}_{algo}_{run_time}.csv"
        pd.DataFrame(labels, columns=["label"]).to_csv(fname, index=False)
        st.download_button(
            "ä¸‹è½½èšç±»æ ‡ç­¾", 
            data=fname.read_bytes(), 
            file_name=fname.name
        )
        
        log_run({
            "dataset": dataset,
            "algo": algo,
            "params": params,
            "ari": ari,
            "file": fname.name,
            "dataset_path": st.session_state.dataset_path
        })
        st.success("å®éªŒå®Œæˆå¹¶è®°å½•åˆ°å†å²")

with tab_history:
    st.header("å†å²è®°å½•")
    history = read_logs()
    
    if not history.empty:
        st.dataframe(history)
        st.subheader("æŒ‰æ•°æ®é›†ç­›é€‰")
        selected_dataset = st.selectbox(
            "é€‰æ‹©æ•°æ®é›†", 
            ["å…¨éƒ¨"] + sorted(history["dataset"].unique())
        )
        if selected_dataset != "å…¨éƒ¨":
            st.dataframe(history[history["dataset"] == selected_dataset])
    else:
        st.info("æš‚æ— å®éªŒè®°å½•ï¼Œå‰å¾€ã€è¿è¡Œå®éªŒã€é€‰é¡¹å¡å¼€å§‹ç¬¬ä¸€ä¸ªå®éªŒ")