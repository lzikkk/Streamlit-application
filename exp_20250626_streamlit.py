# -*- coding: utf-8 -*-
"""
Created on Thu Jun 26 14:57:06 2025

@author: Administrator
"""

# clustering_app.py
import streamlit as st
import pandas as pd
from pathlib import Path
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import adjusted_rand_score, silhouette_score
import matplotlib.pyplot as plt
import os
import re

# ---------- å…¬å…±å‡½æ•° ----------
@st.cache_data
def load_dataset(dataset_name, dataset_path):
    """åŠ è½½æŒ‡å®šæ•°æ®é›†"""
    try:
        # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
        file_path = Path(dataset_path) / f"{dataset_name}.csv"
        print(f"å°è¯•åŠ è½½æ–‡ä»¶: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
        df = pd.read_csv(file_path)
        return df.iloc[:, :-1], df.iloc[:, -1]
    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®é›†æ—¶å‡ºé”™: {str(e)}")
        return None, None

def run_clustering(X, algo, params):
    """æ‰§è¡Œèšç±»ç®—æ³•"""
    if X is None:
        return None
    
    try:
        if algo == "KMeans":
            model = KMeans(n_clusters=params["k"])
        elif algo == "DBSCAN":
            model = DBSCAN(eps=params["eps"], min_samples=params["min_samples"])
        else:  # HAC
            model = AgglomerativeClustering(n_clusters=params["k"])
        labels = model.fit_predict(X)
        return labels
    except Exception as e:
        st.error(f"èšç±»æ‰§è¡Œå‡ºé”™: {str(e)}")
        return None

def evaluate(X, y_true, y_pred):
    """è¯„ä¼°èšç±»ç»“æœ"""
    if X is None or y_true is None or y_pred is None:
        return None, None
    
    try:
        ari = adjusted_rand_score(y_true, y_pred)
        sil = silhouette_score(X, y_pred) if len(set(y_pred)) > 1 else float("nan")
        return ari, sil
    except Exception as e:
        st.error(f"è¯„ä¼°ç»“æœå‡ºé”™: {str(e)}")
        return None, None

def save_labels(dataset, algo, labels):
    """ä¿å­˜èšç±»æ ‡ç­¾"""
    if labels is None:
        return
    
    try:
        Path("results").mkdir(exist_ok=True)
        out = Path("results") / f"{dataset}_{algo}.csv"
        pd.DataFrame(labels, columns=["label"]).to_csv(out, index=False)
        return True
    except Exception as e:
        st.error(f"ä¿å­˜ç»“æœå‡ºé”™: {str(e)}")
        return False

def get_subdirectories(path):
    """è·å–æŒ‡å®šè·¯å¾„ä¸‹çš„å­ç›®å½•åˆ—è¡¨"""
    try:
        if os.path.exists(path):
            return [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]
        return []
    except Exception:
        return []

def find_matching_directories(root_path, current_input):
    """æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•åä½œä¸ºè‡ªåŠ¨è¡¥å…¨å»ºè®®"""
    if not current_input:
        return []
    
    # åˆ†å‰²è·¯å¾„ä¸ºåŸºç¡€è·¯å¾„å’Œæœ€åä¸€éƒ¨åˆ†
    parts = current_input.rstrip(os.sep).split(os.sep)
    if len(parts) <= 1:
        base_path = root_path
        search_term = current_input
    else:
        base_path = os.sep.join(parts[:-1])
        search_term = parts[-1]
    
    # å¦‚æœåŸºç¡€è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•é€çº§å‘ä¸ŠæŸ¥æ‰¾
    while not os.path.exists(base_path) and base_path != "":
        base_path = os.path.dirname(base_path)
    
    # è·å–åŒ¹é…çš„å­ç›®å½•
    subdirs = get_subdirectories(base_path)
    matches = [d for d in subdirs if d.lower().startswith(search_term.lower())]
    
    # æ„å»ºå®Œæ•´è·¯å¾„å»ºè®®
    full_matches = []
    for match in matches:
        if base_path == "":
            full_matches.append(match)
        else:
            full_matches.append(os.path.join(base_path, match))
    
    return full_matches

# ---------- Streamlit é¡µé¢ ----------
st.title("ğŸ§© èšç±»å®éªŒå¹³å° (Streamlit One-File)")

# ä¾§è¾¹æ ï¼šæ•°æ®é›†è·¯å¾„è®¾ç½®
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    # é»˜è®¤æ•°æ®é›†æ ¹ç›®å½•
    DEFAULT_ROOT = "D:\\lzk\\@PG\\03.@G2S2\\è®ºæ–‡ä¿®æ”¹\\0620\\å¯¹æ¯”ç®—æ³•\\Dataset"
    DEFAULT_UCI_PATH = os.path.join(DEFAULT_ROOT, "UCI")
    
    # è‡ªåŠ¨è¡¥å…¨åŠŸèƒ½å®ç°
    st.markdown("### æ•°æ®é›†æ ¹ç›®å½•")
    
    # åˆå§‹åŒ–session_state
    if 'dataset_path' not in st.session_state:
        st.session_state.dataset_path = DEFAULT_UCI_PATH
    
    # ä½¿ç”¨session_stateçš„å€¼ä½œä¸ºè¾“å…¥æ¡†çš„åˆå§‹å€¼
    root_input = st.text_input(
        "", 
        value=st.session_state.dataset_path,
        key="dataset_path_input",
        help="è¾“å…¥æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„ï¼Œæ”¯æŒè‡ªåŠ¨è¡¥å…¨"
    )
    
    # å½“è¾“å…¥æ¡†å€¼æ”¹å˜æ—¶ï¼Œæ›´æ–°session_state
    if root_input != st.session_state.dataset_path:
        st.session_state.dataset_path = root_input
    
    # æ˜¾ç¤ºè‡ªåŠ¨è¡¥å…¨å»ºè®®
    if root_input:
        suggestions = find_matching_directories(DEFAULT_ROOT, root_input)
        if suggestions:
            st.markdown("**è‡ªåŠ¨è¡¥å…¨å»ºè®®:**")
            for i, suggestion in enumerate(suggestions[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªå»ºè®®
                # ç‚¹å‡»æŒ‰é’®æ—¶ï¼Œæ›´æ–°session_stateå¹¶è§¦å‘é‡æ–°æ¸²æŸ“
                if st.button(f"é€‰æ‹©: {suggestion}", key=f"suggestion_{i}"):
                    st.session_state.dataset_path = suggestion
    
    st.header("ğŸ“Š èšç±»è®¾ç½®")
    dataset = st.selectbox(
        "é€‰æ‹©æ•°æ®é›†", 
        ["iris", "glass", "Lsun"],
        help="è¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ•°æ®é›†(éœ€ç¡®ä¿CSVæ–‡ä»¶å­˜åœ¨)"
    )
    algo    = st.selectbox(
        "é€‰æ‹©ç®—æ³•", 
        ["KMeans", "DBSCAN", "HAC"],
        help="è¯·é€‰æ‹©èšç±»ç®—æ³•"
    )
    
    # ç®—æ³•å‚æ•°è®¾ç½®
    if algo == "KMeans" or algo == "HAC":
        k = st.slider("ç°‡æ•° k", 2, 10, 3)
        params = {"k": k}
    else:
        eps = st.slider("eps", 0.1, 5.0, 0.5, step=0.1)
        mns = st.slider("min_samples", 1, 20, 5)
        params = {"eps": eps, "min_samples": mns}

    run_btn = st.button("ğŸš€ è¿è¡Œèšç±»")

# ä¸»é¢æ¿ï¼šæ‰§è¡Œä¸å±•ç¤º
if run_btn:
    # åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨session_stateä¸­çš„è·¯å¾„ï¼‰
    X, y = load_dataset(dataset, st.session_state.dataset_path)
    
    if X is not None and y is not None:
        # æ‰§è¡Œèšç±»
        pred = run_clustering(X, algo, params)
        
        if pred is not None:
            # è¯„ä¼°ç»“æœ
            ari, sil = evaluate(X, y, pred)
            
            if ari is not None and sil is not None:
                st.subheader("ğŸ“Š è¯„ä¼°æŒ‡æ ‡")
                st.write(f"- **ARI**: {ari:.4f}  \n- **Silhouette**: {sil:.4f}")
                
                # äºŒç»´å¯è§†åŒ–ï¼ˆä»…å±•ç¤ºå‰ä¸¤ç»´ï¼‰
                fig, ax = plt.subplots(figsize=(8, 6))
                scatter = ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=pred, cmap="tab10", s=50)
                ax.set_xlabel("Feature 1"); ax.set_ylabel("Feature 2")
                ax.set_title(f"{dataset} | {algo} èšç±»ç»“æœ")
                st.pyplot(fig)
                
                # ä¿å­˜ç»“æœ
                save_status = save_labels(dataset, algo, pred)
                if save_status:
                    st.success("ç»“æœå·²ä¿å­˜åˆ° /results ç›®å½•")
    else:
        st.warning("è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨")